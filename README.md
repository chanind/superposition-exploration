# Superposition Exploration

This is a notebook exploring the phenomenon of superposition in neural networks. We ask: just how many concepts could we fit in a space the size of a typical transformer model's hidden space? And how superposition interference should we expect for a given number of concepts in a given number of dimensions?

The notebook is [exploration.ipynb](exploration.ipynb)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chanind/superposition-exploration/blob/main/exploration.ipynb)
