# Superposition Exploration

This is a notebook exploring the phenomenon of superposition in neural networks. We ask: just how many concepts could we fit in a space the size of a typical transformer model's hidden space? And how superposition interference should we expect for a given number of concepts in a given number of dimensions?

The notebook is in `exploration.ipynb`.
