{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "37e66a4e",
      "metadata": {},
      "source": [
        "# Superposition Exploration\n",
        "\n",
        "It's hypothesized that neural networks represent concepts as nearly-orthogonal vectors in a high-dimensional space, and this lets the network represent many more concepts than there are dimensions in the model's hidden space. This phenomenon is known as the \"[superposition](https://transformer-circuits.pub/2022/toy_model/index.html)\". The [Johnson-Lindenstrauss theorem](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma) proves that as we increase the number of dimensions in a space, the number of almost-orthogonal vectors we can fit in the space increases exponentially.\n",
        "\n",
        "But concretely, how many concepts can we fit in a space the size of a typical transformer model's hidden space? And how superposition interference should we expect for a given number of concepts in a given number of dimensions? In this notebook we'll explore these questions.\n",
        "\n",
        "<img src=\"./assets/superposition.png\" alt=\"Superposition diagram\" width=\"300\">\n",
        "\n",
        "*From [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06877e47",
      "metadata": {},
      "source": [
        "## Defining superposition\n",
        "\n",
        "We need a metric we can use to track how much superposition interference there is in a given space. We'll use **mean max absolute cosine similarity**, $\\rho_\\text{mm}$, as our core measure. For each vector in our space, we calculate the absolute value of cosine similarity between that vector and every other vector in the space, then take the mean across all these max values.\n",
        "\n",
        "$$\n",
        "\\rho_\\text{mm} = \\frac{1}{N} \\sum_{i=1}^{N} \\max_{j \\neq i} \\left| \\frac{\\mathbf{v}_i \\cdot \\mathbf{v}_j}{\\|\\mathbf{v}_i\\| \\|\\mathbf{v}_j\\|} \\right|\n",
        "$$\n",
        "\n",
        "This metric represents a \"worst-case\" measure of superposition interference for each vector in our space."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5f6240d",
      "metadata": {},
      "source": [
        "## Superposition of random vectors\n",
        "\n",
        "If we have $N$ random unit-norm vectors in a $d$-dimensional space, what should we expect $\\rho_\\text{mm}$ to be? We can try this out with a simple simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be7158d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    warnings.warn(\"No GPU or MPS device found, using CPU. This will be very slow!\")\n",
        "\n",
        "\n",
        "def compute_rho_mm(vectors: torch.Tensor, chunk_size: int = 4096) -> float:\n",
        "    norms = vectors.norm(dim=1, keepdim=True)\n",
        "    unit_vectors = vectors / norms\n",
        "    N = unit_vectors.shape[0]\n",
        "\n",
        "    # Process in chunks to avoid materializing the full (N, N) matrix\n",
        "    max_abs_cos = torch.zeros(N, device=vectors.device)\n",
        "    for i in range(0, N, chunk_size):\n",
        "        chunk = unit_vectors[i : i + chunk_size]  # (chunk_size, d)\n",
        "        cos_sim = chunk @ unit_vectors.T  # (chunk_size, N)\n",
        "\n",
        "        # Zero out self-similarities on the diagonal block\n",
        "        diag_start = i\n",
        "        diag_end = min(i + chunk_size, N)\n",
        "        cos_sim[:, diag_start:diag_end].fill_diagonal_(0.0)\n",
        "\n",
        "        max_abs_cos[i : i + chunk_size] = cos_sim.abs().max(dim=1).values\n",
        "\n",
        "    return max_abs_cos.mean().item()\n",
        "\n",
        "\n",
        "dims = [256, 512, 768, 1024]\n",
        "num_vectors = [4 * 1024, 8 * 1024, 16 * 1024, 32 * 1024]\n",
        "\n",
        "\n",
        "results: dict[int, list[float]] = {N: [] for N in num_vectors}\n",
        "for d in tqdm(dims, desc=\"Dimension\"):\n",
        "    for N in tqdm(num_vectors, desc=f\"d={d}\", leave=False):\n",
        "        vectors = torch.randn(N, d, device=device)\n",
        "        rho = compute_rho_mm(vectors)\n",
        "        results[N].append(rho)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "norm = mcolors.LogNorm(vmin=min(num_vectors), vmax=max(num_vectors))\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "for N in num_vectors:\n",
        "    color = cmap(norm(N))\n",
        "    ax.plot(dims, results[N], marker=\"o\", color=color, label=f\"N = {N:,}\")\n",
        "\n",
        "ax.set_xlabel(\"Dimension (d)\")\n",
        "ax.set_ylabel(r\"$\\rho_\\mathrm{mm}$\")\n",
        "ax.set_title(r\"Mean Max Absolute Cosine Similarity ($\\rho_\\mathrm{mm}$) vs Dimension\")\n",
        "\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)\n",
        "cbar.set_label(\"Number of vectors (N)\")\n",
        "cbar.set_ticks(num_vectors)\n",
        "cbar.set_ticklabels([f\"{N:,}\" for N in num_vectors])\n",
        "\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd77668b",
      "metadata": {},
      "source": [
        "If we want to go even bigger, we'll run into trouble. Fortunately, we can compute the expected $\\rho_\\text{mm}$ exactly (up to numerical integration) using the known distribution of cosine similarity between random unit vectors.\n",
        "\n",
        "For two random unit vectors in $\\mathbb{R}^d$, the squared cosine similarity follows a Beta distribution: $\\cos^2\\theta \\sim \\text{Beta}(1/2, (d-1)/2)$. This means the CDF of $|\\cos\\theta|$ is the regularized incomplete beta function:\n",
        "\n",
        "$$P(|\\cos\\theta| \\leq t) = I_{t^2}\\!\\left(\\tfrac{1}{2}, \\tfrac{d-1}{2}\\right)$$\n",
        "\n",
        "For each vector, its max absolute cosine similarity with $N-1$ others has CDF $F(t)^{N-1}$ (treating pairwise similarities as independent, which is excellent for large $d$). The expected value of a non-negative random variable gives us:\n",
        "\n",
        "$$\\mathbb{E}[\\rho_\\text{mm}] = \\int_0^1 \\left(1 - \\left[I_{t^2}\\!\\left(\\tfrac{1}{2}, \\tfrac{d-1}{2}\\right)\\right]^{N-1}\\right) dt$$\n",
        "\n",
        "Let's see how well this matches our simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63428f21",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import integrate\n",
        "from scipy.special import betainc\n",
        "\n",
        "\n",
        "def predicted_rho_mm(N: int, d: int) -> float:\n",
        "    \"\"\"Compute expected rho_mm using exact distribution of cosine similarity.\"\"\"\n",
        "    a, b = 0.5, (d - 1) / 2\n",
        "\n",
        "    def integrand(t: float) -> float:\n",
        "        cdf = betainc(a, b, t**2)\n",
        "        return 1.0 - cdf ** (N - 1)\n",
        "\n",
        "    result, _ = integrate.quad(integrand, 0.0, 1.0)\n",
        "    return result\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "norm = mcolors.LogNorm(vmin=min(num_vectors), vmax=max(num_vectors))\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "dims_smooth = list(range(dims[0], dims[-1] + 1))\n",
        "\n",
        "for N in num_vectors:\n",
        "    color = cmap(norm(N))\n",
        "    # Simulated\n",
        "    ax.plot(dims, results[N], marker=\"o\", color=color, label=f\"N = {N:,} (simulated)\")\n",
        "    # Predicted (exact)\n",
        "    predicted = [predicted_rho_mm(N, d) for d in dims_smooth]\n",
        "    ax.plot(dims_smooth, predicted, color=color, linestyle=\"--\", alpha=0.7, label=f\"N = {N:,} (predicted)\")\n",
        "\n",
        "ax.set_xlabel(\"Dimension (d)\")\n",
        "ax.set_ylabel(r\"$\\rho_\\mathrm{mm}$\")\n",
        "ax.set_title(r\"Simulated vs Predicted $\\rho_\\mathrm{mm}$\")\n",
        "\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)\n",
        "cbar.set_label(\"Number of vectors (N)\")\n",
        "cbar.set_ticks(num_vectors)\n",
        "cbar.set_ticklabels([f\"{N:,}\" for N in num_vectors])\n",
        "\n",
        "ax.legend(fontsize=7, ncol=2)\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a3a30ab",
      "metadata": {},
      "source": [
        "This is a pefect match! Let's use this to see how much superposition interference we should expect for a some really massive numbers of concepts, let's go up to 10 trillion concepts (10^13) and 8192 dimensions, which is the hidden size of the largest current open models. 10 trillion concepts seems like a reasonable upper bound for the max number of concepts that could conceivably be possible, since that would be roughly 1 concept per training token in a typical LLM pretraining run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b02abc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "big_dims = [512, 768, 1024, 2048, 3072, 4096, 6144, 8192]\n",
        "big_N = [10**k for k in range(5, 14, 2)]  # 10^5, 10^7, 10^9, 10^11, 10^13\n",
        "\n",
        "big_results: dict[int, list[float]] = {}\n",
        "for N in tqdm(big_N, desc=\"N values\"):\n",
        "    rho_values: list[float] = []\n",
        "    for d in big_dims:\n",
        "        rho_values.append(predicted_rho_mm(N, d))\n",
        "    big_results[N] = rho_values\n",
        "\n",
        "# Plot: d on x-axis, N as colored lines\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "norm = mcolors.LogNorm(vmin=min(big_N), vmax=max(big_N))\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "for N in big_N:\n",
        "    color = cmap(norm(N))\n",
        "    exp = int(round(np.log10(N)))\n",
        "    ax.plot(big_dims, big_results[N], marker=\"o\", markersize=4, color=color, label=f\"N = $10^{{{exp}}}$\")\n",
        "\n",
        "ax.set_xlabel(\"Dimension (d)\")\n",
        "ax.set_ylabel(r\"$\\rho_\\mathrm{mm}$\")\n",
        "ax.set_title(r\"Superposition interference ($\\rho_\\mathrm{mm}$) vs hidden dimension\")\n",
        "\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)\n",
        "cbar.set_label(\"Number of vectors (N)\")\n",
        "\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592c494c",
      "metadata": {},
      "source": [
        "*10 trillion concepts* in 8192 dimensions has far less superposition interference than just *100K concepts* in 768 dimensions (the hidden dimension of GPT-2)! Also, at any given dimension, increasing the number of concepts by 100x doesn't even increase superposition interference much!\n",
        "\n",
        "<h1 align=\"center\" style=\"font-size: 100px;\">ðŸ¤¯</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1875b4e5",
      "metadata": {},
      "source": [
        "## What if we optimally placed the vectors instead?\n",
        "\n",
        "Everything above assumes random unit vectors. But what if we could arrange them optimally â€” placing each vector to minimize the worst-case interference? Would we do significantly better?\n",
        "\n",
        "From [spherical coding theory](https://en.wikipedia.org/wiki/Spherical_code), the answer is: **barely**. The minimum achievable max pairwise correlation for $N$ optimally-placed unit vectors in $d$ dimensions is given by the [spherical cap packing bound](https://en.wikipedia.org/wiki/Spherical_cap#Solid_angle):\n",
        "\n",
        "$$\\varepsilon_\\text{optimal} \\approx \\sqrt{1 - N^{-2/(d-1)}}$$\n",
        "\n",
        "The intuition is that each vector \"excludes\" a spherical cap around itself, and we're counting how many non-overlapping caps fit on the unit sphere in $\\mathbb{R}^d$.\n",
        "\n",
        "When $\\ln N \\ll d$ (which holds for all practical settings â€” even $N = 10^{13}$ in $d = 512$ gives $\\ln N / d \\approx 0.06$), we can Taylor-expand:\n",
        "\n",
        "$$N^{-2/(d-1)} = e^{-2\\ln N/(d-1)} \\approx 1 - \\frac{2\\ln N}{d}$$\n",
        "\n",
        "which gives:\n",
        "\n",
        "$$\\varepsilon_\\text{optimal} \\approx \\sqrt{\\frac{2\\ln N}{d}}$$\n",
        "\n",
        "This is exactly the leading-order term of the random vector formula! So random placement is already near-optimal â€” there's essentially nothing to gain from clever geometric arrangement of the vectors, at least for the $N$ and $d$ values relevant to real neural networks.\n",
        "\n",
        "This is a remarkable consequence of high-dimensional geometry: in spaces with hundreds or thousands of dimensions, random directions are already so close to orthogonal that you can't do meaningfully better by optimizing."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
